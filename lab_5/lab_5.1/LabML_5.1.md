Лабораторная работа №5.1

**Применение модели решающих деревьев в задачах классификации**

Для выполнения данной работы мы будем использовать набор данных (датасет) с информацией о медицинских параметрах больных диабетом. Датасет может быть загружена непосредственно из scikit-learn.

from sklearn.datasets import load_diabetes()

Характеристики набора данных можно посмотреть самостоятельно, например так:

diabetes = datasets.load_diabetes()

print(diabetes['DESCR'])

или прочитать по адресу: <https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset>

Задачи к лабораторной работе:

1. Решить задачу классификации больных методом логистической регрессии и решающих деревьев используя стандартные настройки моделей. Вывести стандартные метрики. Сделать вывод на основе анализа метрик о том, какая из рассмотренных моделей более подходит для исследуемого датасета.
2. Исследовать значение выбранной самостоятельно метрики (выбор обосновать) в зависимости от глубины решающего дерева. Построить график зависимости.
3. Для модели с оптимальной глубиной полученной в задании 2, отрисовать получившееся дерево (см. пример далее), важность признаков (feature importances) в виде столбчатой диаграммы, а также PR и ROC кривые
4. Опциональное задание: аналогично п.2 исследовать зависимость метрики от других параметров модели решающего дерева, например **max_features.**

# Пример в Scikit-learn

from sklearn.datasets import load_iris
from sklearn import tree

\# Load in our dataset
iris_data = load_iris()

\# Initialize our decision tree object
classification_tree = tree.DecisionTreeClassifier()

\# Train our decision tree (tree induction + pruning)
classification_tree = classification_tree.fit(iris_data.data, iris_data.target)

Scikit-learn также позволяет визуализировать дерево с помощью библиотеки graphviz, в которой есть несколько очень полезных опций для визуализации узлов решений и разбиений, выученных моделью. Ниже обозначим узлы разными цветами, отталкиваясь от признаков имен, и отобразим класс и признак каждого узла.

import graphviz
dot_data = tree.export_graphviz(classification_tree, out_file=None,
`                     `feature_names=iris.feature_names,  
`                     `class_names=iris.target_names,  
`                     `filled=True, rounded=True,  
`                     `special_characters=True)  
graph = graphviz.Source(dot_data)  
graph.render("iris")

![](Aspose.Words.283d0424-f04c-4e65-9272-1253a1e503ac.001.png)

Кроме того, в Scikit-learn можно указать несколько параметров для модели дерева решений. Ниже приведем некоторые из таких параметров, позволяющих получить лучший результат:

**max_depth:** максимальная глубина дерева — точка, на которой останавливается разбиение узлов. Меньшее количество сделает модель быстрой, но не точной. Большее количество увеличивает точность, но создает риски переобучения и замедляет процесс.

**min_samples_split:** необходимое минимальное количество выборок для разбиения узлов.

**max_features:** число признаков для поиска лучшей точки для разбиения. Чем больше число, тем лучше результат. Но в этом случае обучение займет больше времени.

**min_impurity_split:** порог для ранней остановки роста дерева. Узел разобьется только в том случае, если его точность будет выше указанного порога. Такой метод может служить в качестве компромисса между минимизацией переобучения (высокое значение, маленькое дерево) и высокой точностью (низкое значение, большое дерево).

**presort:** выбор того, нужно ли предварительно сортировать данные для ускорения поиска наилучшего разбиения при подборе. Если данные заранее отсортируются по каждому признаку, то алгоритму обучения будет гораздо проще найти хорошие значения для разбиения.
